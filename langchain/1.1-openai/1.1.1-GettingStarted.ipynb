{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # pyright: ignore[reportMissingImports]\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x14098fbe0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x141176a70> root_client=<openai.OpenAI object at 0x112b24280> root_async_client=<openai.AsyncOpenAI object at 0x14098f340> model_name='openai/gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://models.github.ai/inference'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"openai/gpt-4o\", base_url=\"https://models.github.ai/inference\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence models designed to create new, original content based on patterns and examples in training data. It uses machine learning techniques to generate outputs such as text, images, videos, audio, or even code that resembles human-created content. The key feature of generative AI is its ability to produce outputs that are not directly copied from its training data but synthesized based on learned structures and relationships.\\n\\n### How Does Generative AI Work?\\nGenerative AI models typically rely on advanced algorithms, including **neural networks**, to process and learn from massive datasets. Common approaches include:\\n\\n1. **Generative Adversarial Networks (GANs)**:\\n   GANs consist of two neural networks—one that generates content (the generator) and another that evaluates it (the discriminator). The two networks compete, gradually improving the quality of the generated content.\\n\\n2. **Transformer Models**:\\n   Models like **GPT (Generative Pre-trained Transformer)**, **DALL·E**, and others use transformers, which are designed to process sequences of data (e.g., text or images). They are trained on extensive datasets using self-supervised learning to predict outcomes based on context.\\n\\n3. **Diffusion Models**:\\n   Used in generative image tools like **Stable Diffusion**, these models gradually transform random noise into coherent data (e.g., realistic images) based on learned patterns.\\n\\n---\\n\\n### Applications of Generative AI\\nGenerative AI has revolutionized creativity and automation across many fields. Examples include:\\n- **Text Generation**: Writing articles, blogs, and reports (e.g., ChatGPT, GPT-4).\\n- **Image Creation**: Generating unique artwork or illustrations (e.g., DALL·E, MidJourney).\\n- **Video Generation**: Producing custom animations or deepfakes.\\n- **Code Generation**: Assisting programmers with auto-suggestive coding (e.g., GitHub Copilot).\\n- **Music Composition**: Creating new melodies or songs.\\n- **Game Design**: Building characters, environments, and storylines procedurally.\\n- **Simulations**: Creating virtual worlds or structures for research and development.\\n\\n---\\n\\n### Benefits vs. Challenges\\n#### **Benefits**:\\n- Automates content creation, saving time and effort.\\n- Enhances creativity by providing new ideas or inspirations.\\n- Enables personalization at scale, such as marketing campaigns tailored to individuals.\\n\\n#### **Challenges**:\\n- Ethical concerns, including the potential for generating misleading information.\\n- Copyright and ownership issues.\\n- Risk of bias, as AI models can reflect biases present in training data.\\n- High computational costs to train and deploy advanced models.\\n\\n---\\n\\nIn summary, generative AI is transforming the way we create and interact with content, unlocking opportunities for innovation while also raising important ethical and societal considerations.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 569, 'prompt_tokens': 13, 'total_tokens': 582, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CSPM4Az2rQHljykODqZWeZZrig03Y', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--027c8574-bb3b-4764-af5f-772a6a1cff3d-0' usage_metadata={'input_tokens': 13, 'output_tokens': 569, 'total_tokens': 582, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Langsmith is a debugging, testing, and monitoring tool for building **LangChain** and LLM (Large Language Model)-based applications. It assists developers in optimizing their applications by providing deeper insights into how LLMs behave in various scenarios, enabling enhanced performance and reliability. Langsmith aims to streamline the development and workflow of LLM-powered apps by offering tools for experimenting with prompts, managing datasets, and analyzing outputs.\\n\\n### Key Features of Langsmith:\\n1. **Prompt Experimentation**: Helps developers test various LLM prompts interactively, fine-tune them, and understand how slight changes affect the model's performance.\\n   \\n2. **Dataset Management**: Developers can create and manage evaluation datasets to repeatedly test their LLM application against them, and refine accordingly.\\n   \\n3. **Tracing & Debugging**: Langsmith provides a unified platform to trace the flow of user inputs, intermediate steps, LLM calls, and other operations to debug errors and performance issues.\\n\\n4. **Evaluation Metrics**: Offers automated tools that allow you to benchmark outputs from different prompts or models, making it easier for developers to monitor accuracy, relevancy, and quality.\\n\\n5. **Monitoring**: Tracks the operational performance of apps after deployment, ensuring consistent behavior in real-world usage.\\n\\nLangsmith is part of the broader **LangChain** ecosystem, which is a popular framework for building context-aware AI applications using LLMs. Langsmith enables developers to refine their applications quickly and ensure they’re robust before scaling. As of 2023, it has become a go-to choice for many developers working with language models like GPT, Claude, and other generative AI tools.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 334, 'prompt_tokens': 33, 'total_tokens': 367, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CSPMFXnLVYc6sszHCflUYksf9YOMZ', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--4ae8ee5e-81c4-49ba-9293-9b921e2dbad5-0' usage_metadata={'input_tokens': 33, 'output_tokens': 334, 'total_tokens': 367, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my training cutoff in October 2023, **LangSmith** is a platform introduced by **LangChain**, a popular framework used for building applications with large language models (LLMs). LangSmith focuses on **debugging, monitoring, and evaluation** of applications that leverage LLMs, which is crucial for ensuring the reliability and performance of AI-driven systems.\n",
      "\n",
      "Here are some key features of LangSmith:\n",
      "\n",
      "1. **Debugging LLM Applications**:  \n",
      "   LLM applications often involve complex prompt designs and interactions with external tools. LangSmith provides developers with tools to inspect and debug workflows in detail. It allows visibility into how models are being used, the intermediate outputs at each step, and where bottlenecks or failures occur.\n",
      "\n",
      "2. **Monitoring and Observability**:  \n",
      "   LangSmith helps track the performance of LLM-based applications in production by offering observability features. Developers can monitor the execution of chains (sequences of prompts or tasks) and agents, identifying trends and areas for optimization.\n",
      "\n",
      "3. **Evaluation Framework**:  \n",
      "   LangSmith allows for systematic evaluation of LLM outputs. With both automated and human-in-the-loop evaluations, developers can measure the quality, accuracy, and relevancy of the responses generated by their language models.\n",
      "\n",
      "4. **Integration with LangChain**:  \n",
      "   Since LangSmith is part of the LangChain ecosystem, it tightly integrates with LangChain's tools, chains, and agents. This makes it easy for developers already using LangChain to incorporate LangSmith for better application monitoring and improvement.\n",
      "\n",
      "5. **Scalable for Production Apps**:  \n",
      "   LangSmith is designed for applications in production, providing scalable tools for testing and monitoring workflows with high reliability.\n",
      "\n",
      "### Why It's Important\n",
      "In the realm of generative AI and LLMs, where models can produce variable outputs, ensuring consistent and high-quality performance is challenging. LangSmith addresses these challenges by giving developers the ability to test, understand, improve, and maintain large-scale applications effectively.\n",
      "\n",
      "For more details or the latest updates, it would be best to check LangChain’s official documentation or their announcements regarding LangSmith.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
